:tabsize: 8
:toc: left
:toclevels: 5

= IPD 20 Kernel Test Facility

|===
|Authors |State

|Ryan Zezeski <ryan@zinascii.com>
|draft
|===


== Goal

Add a general testing facility to the kernel to allow for in situ
kernel testing.

== Background

Testing is a vital part of any well maintained program, and a kernel,
along with its various modules, should be no exception. But testing
kernel modules can be hard, especially if trying to exercise specific
branches in specific functions. For the most part we rely on clever
userspace tests, in the form of `usr/src/test`, to exercise our most
important code paths. While this can be made to work it also requires
additional effort (as compared to what I'm about to propose) from the
developer, if it can be made to work at all. Furthermore, these tests
may be at the mercy of several layers of abstraction on top of the
actual system they are trying to test. In short, it can sometimes feel
like trying to drain a five gallon bucket with a straw.

For many of us coming to kernel development for the first time there
is often a moment of clarity when we realize the kernel is just
another program. Sure, it's a "special" program, responsible for
controlling the hardware and bringing order to the city of userspace
programs, but at the end of the day it's a program nonetheless. It's
mostly written in C, and C programs are just collections of functions,
and it turns out those functions can often be tested directly. But the
big eureka moment, at least for me, was to realize that to properly
test kernel functions you need to be _in the kernel_ -- and thus the
idea of ktest was born.

== Proposal

The kernel test facility (from here on out simply referred to as
ktest) provides a means for in situ kernel testing: that is, running
tests against kernel functions from inside the kernel itself. This
facility provides a number of services.

1. The `ktest` kernel module which acts as a central location for all
   administration and execution of test modules.

2. A kernel API for writing test modules. Allowing them to register
   tests with the `ktest` kernel module.

3. A `ktest` pseudo device which presents userland control of the
   `ktest` module.

4. A `ktest` user command which abstracts away the details of the
   pseudo device, allowing a user to easily load, list, and run tests.

The rest of this section is spent discussing the architecture and API
of ktest. But before going further, it's worth just seeing what running
`ktest` looks like.

.run all registered tests
----
# ktest run '*'
RESULT LINE  MODULE      SUITE           TEST
NONE   0     ktest       selftest        ktest_st_none_test
PASS   24    ktest       selftest        ktest_st_pass_test
FAIL   36    ktest       selftest        ktest_st_fail_test
ERROR  66    ktest       selftest        ktest_st_err_test
SKIP   75    ktest       selftest        ktest_st_skip_test
ERROR  109   ktest       selftest        ktest_st_mult_result_test
PASS   247   mac         checksum        mac_sw_cksum_ipv4_tcp_test
PASS   302   mac         checksum        mac_sw_cksum_ipv4_bad_proto_test
PASS   31    stream      mblk            mblkl_test
PASS   54    stream      mblk            msgsize_test


MODULE/SUITE                            TOTAL PASS  FAIL  ERR   SKIP  NONE
ktest                                   6     1     1     2     1     1
--------------------------------------------------------------------------
  selftest                              6     1     1     2     1     1

mac                                     2     2     0     0     0     0
--------------------------------------------------------------------------
  checksum                              2     2     0     0     0     0

stream                                  2     2     0     0     0     0
--------------------------------------------------------------------------
  mblk                                  2     2     0     0     0     0
----

=== The Test Triple

The currency in the world of ktest is the test. Test modules register
tests; the ktest facility lists and executes them. But while ktest may
be testing C functions, that doesn't mean it must also live in the
land of the flat namespace. Instead, ktest provides a three-level
namespace, also known as the "test triple".

.the test triple
----
<module>:<suite>:<test>
----

Module:: The top of the namespace, typically named after the
module-under-test. For example, a test module built for testing the
`mac` module would use `mac` as the module name. However, there is no
hard rule that a test module must be named after its
module-under-test, it's merely a suggestion. Such a convention is a
bit unwieldy for mega modules like `genunix`. In those cases it makes
sense to break from the norm.

Suite:: Each module consists of one or more suites. A suite groups
tests of related functionality. For example, you may have several
tests that verify checksum routines for which you might name the suite
`checksum`.

Test:: The name of the test. This can be any string which you find
descriptive of the test. A unit test for a single, small function will
often use the name of the function-under-test with a `_test` suffix
added. But for testing a series of function calls, or a larger
function, it may make more sense to abandon this simple convention. An
additional degree of freedom offered is that the test name does not
have to match the function name in the test module.

The test triple can be partially or fully-qualified, depending on the
context. A fully-qualified triple is one that precisely names one
test, by virtue of specifying each level of the namespace -- it's
unambiguous. A partially-qualified triple, on the other hand, can be
ambiguous; it only names some of the namespace or makes use of globs
in the namespace.

.fully-qualified triple
----
mac:checksum:mac_sw_cksum_ipv4_tcp_test
----

.partially-qualified triples
----
*
*:*:*
mac:
mac:checksum
mac:*:mac_sw*
----

=== The Context Handle

All communication between ktest and the individual test happens via
the "context object". This object cannot be accessed directly.
Instead, ktest provides a context handle to be accessed via its
`ktest(9F)` API. A test must conform to the following prototype.

.test prototype
----
typedef void (*ktest_fn_t)(ktest_ctx_hdl_t *ctx);
----

=== Setting Test Results

The entire point of a test is to convey a result to the user.
Typically this is a result of pass or fail: pass implies the test ran
as expected and all conditions were satisfied; fail implies a
condition was violated. A test may also indicate a result of error or
skip. All functions require a handle to the context and the line
number from which the result function is being called (by making use
of the `\\__LINE__` macro).

`ktest_result_pass(ktest_ctx_hdl_t *, int)`:: Indicates that the test
ran as expected and all conditions were met.

`ktest_result_fail(ktest_ctx_hdl_t *, int, const char *, ...)`::
Indicates a condition was violated. The test should provide a message
describing the condition that failed and why it failed.

`ktest_result_error(ktest_ctx_hdl_t *, int, const char *, ...)`::
Indicates that the test encountered an _unexpected_ error. An
unexpected error is one that is not directly related to the logic the
test is trying to exercise. This may be failure to acquire needed
resources or failure caused by some system not directly related to
what you are testing. These will be most typical in setup code that
may need to interact with the kernel at large in order to setup the
context needed for your specific test. Importantly, it's a condition
which stops the test from making its pass/fail assessment.

`ktest_result_skip(ktest_ctx_hdl_t *, int, const char *, ...)`::
Indicates that the test lacks the required context to execute. The
reasons for skipping will vary, but typically it indicates lack of
resources or specific hardware needed for the test. This is similar to
an error result, with the twist that the test preemptively decides it
cannot run in its current environment.

==== KTest Result Macros

The API descirbed above requires repeitive use of the `\\__LINE__`
macro. The KTest result macros hide this verbosity and are preferred.

* `KT_PASS(ctx)`
* `KT_FAIL(ktext_ctx_hdt_t *ctx, char *msg, ...)`
* `KT_ERROR(ktext_ctx_hdt_t *ctx, char *msg, ...)`
* `KT_SKIP(ktext_ctx_hdt_t *ctx, char *msg, ...)`

==== KTest ASSERT Macros

NOTE: I decided to cut the `KT_ASSERTB*` variants from the
implementation as I found no initial use case for them.

Even with the help of the `KT_*` macros, writing test assertions
requires quite a bit of verbosity and boilerplate; requiring an if
statement, a `KT_*` call, and the failure message arguments. The KTest
ASSERT macros provide an `ASSERT3`-like family of macros to reduce the
boilerplate and make test writing feel more natural. However, they are
different from the `ASSERT3` family in three ways.

1. They all require the additional context argument in order to set
   the failure result when the assert trips.

2. They do not panic but instead build a failure message, call
   `ktest_result_fail()`, and cause an immediate return of the test
   function.

3. The "goto" and "block" variations of these macros provide the
   ability to cleanup test state instead of returning immediately.

[cols="45%,55%"]
|===
|Prototype |Description

2+^h|KTest ASSERT

|`KT_ASSERT3S(left, op, right, ctx)` +
`KT_ASSERT3U(left, op, right, ctx)` +
`KT_ASSERT3P(left, op, right, ctx)` +
`KT_ASSERT(exp, ctx)` +
`KT_ASSERT0(exp, ctx)` +

|The most direct translation of the ASSERT3 family of macros. Each
 takes an additional argument at the end, specifying the context
 handle passed to the test function. This is used by the macro to set
 the appropriate failure condition inside the context object. These
 macros offer no way to cleanup test resources.

2+^h|KTest ASSERT Goto

|`KT_ASSERT3SG(left, op, right, ctx, label)` +
`KT_ASSERT3UG(left, op, right, ctx, label)` +
`KT_ASSERT3PG(left, op, right, ctx, label)` +
`KT_ASSERTG(exp, ctx, label)` +
`KT_ASSERT0G(exp, ctx, label)` +

|These macros are like the KTest ASSERT macros, but after setting the
 `ctx` they jump to `label`. This allows one to provide a common
 cleanup routine under the guise of a label, which can then be shared
 by multiple asserts.

2+^h|KTest ASSERT Block

a|----
KT_ASSERT3SB(left, op, right, ctx) {
    ...
}
KT_ASSERTB_END
----

----
KT_ASSERT3UB(left, op, right, ctx) {
    ...
}
KT_ASSERTB_END
----

----
KT_ASSERT3PB(left, op, right, ctx) {
    ...
}
KT_ASSERTB_END
----

----
KT_ASSERTB(exp, ctx) {
    ...
}
KT_ASSERTB_END
----

----
KT_ASSERT0B(exp, ctx) {
    ...
}
KT_ASSERTB_END
----

|These macros are like the KTest ASSERT macros, but after setting the
 `ctx` they run the code inside the trailing block. The trailing block
 MUST be followed by a `KT_ASSERTB_END`. This is useful for one-off
 cleanup or whenever using a label is not possible or would result in
 more complicated code.
|===

All macros listed above also have a corresponding KTest ERROR macro,
in the form of `KT_EASSERT*`. The difference being that these asserts
set an error result when tripped.

==== Additional failure context

Sometimes the failure message generated by the `KT_ASSERT` macros is
not enough. You might want to prepend some information to the message
to provide additional context about the failure. This would require
using the ktest result API manually, which defeats the purpose of the
`KT_ASSERT` macros. Instead, ktest offers the
`ktest_msg_{prepend,clear}(9F)` API; allowing you to prepend
additional context to the failure message (if the assertion should
trip) while still using the `KT_ASSERT` macros.

For example, if you were asserting an invariant on an array of
objects, and you wanted the failure message to include the index of
the object which tripped the assert, you could write something like
the following.

.prepend/clear API
----
for (int i = 0; i < num_objs; i++) {
        obj_t *obj = &objs[i];

        ktest_msg_prepend(ctx, "objs[%d]: ", i);
        KT_ASSERT3P(obj->o_state, !=, NULL, ctx);
}

ktest_msg_clear(ctx);
----

=== Test Input

A test has the option to require input. The input is always in the
form of a byte stream. The interpretation of those bytes is left to
the test. The ktest facility treats the input stream as opaque with
the exception of requiring that it be at least one byte in length.

A user specifies an input stream by way of a path on the local
filesystem. The `ktest(8)` command will attempt to read this file in
its entirety and pass the byte stream into the ktest kernel module.
Ktest provides an API for the test to get a pointer to the byte
stream, along with its length.

.Input API
----
void ktest_get_input(const ktest_ctx_hdl_t *ctx, uchar_t *input, size_t *len)
----

=== Testing Private Functions

A test module that can't test `static` functions is going to be
severely limited in its usefulness. After all, these are often the
functions doing some of the most important work, and are most likely
to be amenable to testing -- in that they often rely less on global
context and more on their arguments. However, as they are `static`
functions, their linkage is limited to that of the module-under-test.
The ktest facility works around this by dynamically loading the
function object into the test module via another set of `ktest(9F)`
APIs.

.APIs for `static` function access
----
int ktest_hold_mod(const char *module, ddi_modhandle_t *hdl)
int ktest_get_fn(ddi_modhandle_t hdl, const char *fn_name, void **fn)
void ktest_release_mod(ddi_modhandle_t hdl)
----

The test module must perform four steps when accessing a `static`
function.

1. The test module must recreate the function prototype in order for
   it to properly make use of the function pointer. This is probably
   best done as a `typedef`. For each test function that makes use of
   this function, the test module should declare a local variable to
   hold the function pointer, using the `typedef`.

2. The test module must get a handle to the module-under-test in order
   to use the `ddi_modsym(9F)` API. This is done via
   `ktest_hold_mod(9F)`. Acquiring this handle also puts a hold on the
   module, and thus the API is framed in such a way as to remind the
   user to perform the subsequent release.

3. The test module must fill in the function pointer via
   `ktest_get_fn(9F)`, after which the function pointer may be used
   the same as it would be in the module-under-test.

4. The test module must release the module handle via
   `ktest_release_mod(9F)`.

The typical pattern looks something like the following.

.using a `static` function in a test module
----
typedef boolean_t (*mac_sw_cksum_ipv4_t)(mblk_t *, uint32_t, ipha_t *,
    const char **);

void
mac_sw_cksum_ipv4_tcp_test(ktest_ctx_hdl_t *ctx)
{
	ddi_modhandle_t hdl = NULL;
	mac_sw_cksum_ipv4_t mac_sw_cksum_ipv4 = NULL;

	<... snip ...>

	if (ktest_hold_mod("mac", &hdl) != 0) {
		KT_ERROR(ctx, "failed to hold 'mac' module");
		return;
	}

	if (ktest_get_fn(hdl, "mac_sw_cksum_ipv4",
	   (void **)&mac_sw_cksum_ipv4) != 0) {
		KT_ERROR(ctx, "failed to resolve symbol %s`%s",
		    "mac", "mac_sw_cksum_ipv4");
		goto cleanup;
	}

	<... snip ...>

	KT_ASSERTG(mac_sw_cksum_ipv4(mp, ehsz, ip, &err), ctx, cleanup);

	<... snip ...>

cleanup:
	if (hdl != NULL) {
		ktest_release_mod(hdl);
	}

	<... snip ...>
}
----

=== Registering Tests

The ktest facility tracks tests through various private objects which
store the required information needed for each module, suite, and
test. Once again the test module cannot access these objects directly,
but rather interacts with them through opaque handles. The creation and
registration of these objects is done through the `ktest(9F)` API
described below. A test module should typically perform registration
as part of its `_init()` callback.

`int ktest_create_module(char *name, char *mod, ktest_module_hdl_t **out)`::
Create a new test module named `name`, which tests the module named
`mod`. Place the resulting module object in `*out`.

`int ktest_create_suite(char *name, ktest_suite_hdl_t **out)`::
Create a new suite named `name` and place it in `*out`.

`int ktest_add_test(ktest_suite_t *ks, char *name, ktest_fn_t fn, ktest_test_flags_t flags)`::
Create a new test named `name` and add it to the suite object `ks`.
This test will run the test function `fn` when executed.

`int ktest_add_suite(ktest_module_hdl_t *km, ktest_suite_hdl_t *ks)`:: Add the
test suite `ks` to the test module `km`.

`void ktest_register_module(ktest_module_hdl_t *km)`:: Register the
test module with the ktest facility. This is the last call made, after
all the tests/suites are created and added to the test module object.

|===
|Flag |Semantic

|KTF_NONE
|No flags.

|KTF_INPUT
|This test requires an input stream.

|===

=== Test Module Packaging and Location

The ktest facility does not dictate where your test modules live,
either in their source or binary form, nor how those modules are
loaded. The facility's goal is to provide a means for registering,
listing, and executing tests, but not necessarily dictate all the
terms and conditions of how that is done. That said, there are general
conventions that we should strive to follow.

Test modules should be dedicated, misc-type loadable kernel modules,
separate from the module-under-test. They should use `modlmisc`
linkage and perform test registration/deregistration in their
`_init(9E)` and `_fini(9E)` callbacks. A given test module will
typically live adjacent to its module-under-test in the `usr/src/uts`
tree. The source file and binary should generally use the name
`<module-under-test>_test`. You should deviate from this rule when the
module-under-test covers many subsystems and having multiple test
modules would add clarity. For example, the mblk routines in the
"STREAMS subsystem" are part of `genunix`. But `genunix` covers a lot
of ground, and `genunix_test.c` would be a pretty big source file. It
makes more sense to create a `stream_test.c` next to the `stream.c`
file and create a `stream_test` module that exercises the various
stream APIs in `genunix`.

Test modules, like system libraries, should come welded to the system
-- the source code for the test module should live in illumos-gate.
The main exception would be a test delivered as part of an out-of-gate
driver or for downstream distributions testing their own kernel
functionality (though in that case it should be in their downstream
gate).

Delivering test modules is a choice left to each downstream
distribution. That said, we must decide how to structure the IPS
manifests in gate. First, it makes sense to give the ktest facility
its own package. This package should include the driver, the
`ktest(8)` command, public header files, and relevant man pages.
Things get more interesting when determining how test modules should
be delivered. The following is a table of potential options and their
trade-offs.

|===
|Delivery| Trade-offs

|1. All in-gate tests delivered in ktest package.
a|* One package gives you everything.
* No test modules delivered unless you absolutely want them.
* Delivers test modules for modules that may not be attached and that
have no relevance to your system .

|2. The test module is delivered with same package that delivers the
 module-under-test and its package depends on the ktest package.
a|* Only the necessary test modules are installed.
* Given that at least one module-under-test is part of the main kernel
  (like genunix), this effectively means ktest is always delivered.

|3. Same as previous, but don't require ktest dependency.
a|* Same benefits as above.
* The ktest package is only installed if the user requests it.
* Tests are installed even when there is no means to run them.

|4. Same as previous, but add a `ktest` facet for the test modules.
a|* Same benefits as above.
* Tests are installed only when the user explicitly enables the
  `ktest` facet.

|5. A mix of all of the above. Deliver "core" test modules with the
 ktest package. Deliver "non-core" test modules with the
 module-under-test's package and only if the `ktest` facet is enabled.
 a|* One package gives you all core tests.
* No package depends on ktest.
* Installing non-core test modules requires additional opt-in via
  `ktest` facet.
|===

Originally I wanted to go with option (3), but after some more thought
I think we should go with option (5). We deliver "core" test modules
(core modules are ones that are always installed regardless of
platform) as part of the ktest package and "non-core" test modules are
delivered via the same package as the module-under-test. This prevents
us from delivering test modules for modules that don't exist on the
system. Furthermore, installing non-core test modules requires an
additional opt-in via the `ktest` facet.

As these test modules are misc-type modules, they are delivered in the
`misc` module directory. However, in order not to pollute the `misc/`
directory, they are placed in their own `ktest/` subdirectory.

.ktest test modules home
----
/usr/kernel/misc/ktest/amd64
----

=== ktest(8)

The `ktest(8)` command controls all interactions between the user and
ktest facility, as well as all interactions between the test modules
and ktest facility. That is, unless done through some other means like
`modload`, all test module loading, unloading, listing, and running
should only occur as a direct result of executing the `ktest` command.

The ktest device may only be accessed from the Global Zone by a
process with the `PRIV_SYS_DEVICES` privilege. While ktest is primarily
meant as a development tool for a development environment, you could
also use it as a health check for a production system during
pre-flight. For that reason the ktest device does not allow arbitrary
users to access it given it's essentially a vector to execute
arbitrary code you want in the kernel (much like any use of
`add_drv(8)` or `modload(8)`).

.ktest usage
----
# ktest <subcommand> [cmd_opts] [args]
----

.common options
|===
|Option| Description

a|`-H`
a|Elide the column headers.

a|`-o`
a|Select the fields you wish to output.

a|`-p`
a|Write output in a machine-friendly parsable format where each column
is separated by a colon (`:`) character. The `-o` option is required
with this option in order to protect against potential future field
additions or output reordering. The idea is to use the `-Hpo
field,...` options when scripting ktest.

|===

==== Loading/Listing Test Modules

[NOTE]
====
After several false starts around test module listing/loading, I
realized that it's not something that ktest should implement. This is
a job best left to `modload(8)` and friends.
====

==== Listing Tests

The `list` command lists all registered tests. One or more triples may
be specified to narrow the listing.

.ktest list usage
----
ktest list [-H] [[-p] -o field[,...]] [triple]...
----

.list all tests
----
# ktest list
MODULE      SUITE           TEST                                         INPUT
ktest       selftest        ktest_st_none_test                           N
ktest       selftest        ktest_st_pass_test                           N
ktest       selftest        ktest_st_fail_test                           N
ktest       selftest        ktest_st_err_test                            N
ktest       selftest        ktest_st_skip_test                           N
ktest       selftest        ktest_st_input_test                          Y
ktest       selftest        ktest_st_mult_result_test                    N
mac         checksum        mac_sw_cksum_ipv4_tcp_test                   N
mac         checksum        mac_sw_cksum_ipv4_bad_proto_test             N
mac         checksum        mac_sw_cksum_ipv4_snoop_test                 Y
stream      mblk            mblkl_test                                   N
stream      mblk            msgsize_test                                 N
----

==== Running Tests

The `run` command executes registered tests and reports their results.

.ktest run usage
----
ktest run [-H] [[-p] -o field[,...]] [-i input] triple...
----

.run options
|===
|Option| Description

a|`-i <input stream file>`
a|Specify the file to act as the input stream for all tests requiring input.

|===

The simplest thing you can do is run all registered tests. Unlike the
`list` command, the `run` command does not assume you want to run all
tests if given no input. Rather, it always requires an explicit input
to avoid the accidentally running of all tests. But running all tests
is still easy enough, just pass the `*` triple.

.run all tests
----
# ktest run '*'
RESULT LINE  MODULE      SUITE           TEST
NONE   0     ktest       selftest        ktest_st_none_test
PASS   24    ktest       selftest        ktest_st_pass_test
FAIL   36    ktest       selftest        ktest_st_fail_test
ERROR  66    ktest       selftest        ktest_st_err_test
SKIP   75    ktest       selftest        ktest_st_skip_test
ERROR  109   ktest       selftest        ktest_st_mult_result_test
PASS   247   mac         checksum        mac_sw_cksum_ipv4_tcp_test
PASS   302   mac         checksum        mac_sw_cksum_ipv4_bad_proto_test
PASS   31    stream      mblk            mblkl_test
PASS   54    stream      mblk            msgsize_test


MODULE/SUITE                            TOTAL PASS  FAIL  ERR   SKIP  NONE
ktest                                   6     1     1     2     1     1
--------------------------------------------------------------------------
  selftest                              6     1     1     2     1     1

mac                                     2     2     0     0     0     0
--------------------------------------------------------------------------
  checksum                              2     2     0     0     0     0

stream                                  2     2     0     0     0     0
--------------------------------------------------------------------------
  mblk                                  2     2     0     0     0     0
----

To run a single test which requires an input stream you can use the
`-i` option.

.pass input to single test
----
# ./ktest run -i ~/ssh.pcap mac:checksum:mac_sw_cksum_ipv4_snoop_test
RESULT LINE  MODULE      SUITE           TEST
PASS   627   mac         checksum        mac_sw_cksum_ipv4_snoop_test


MODULE/SUITE                            TOTAL PASS  FAIL  ERR   SKIP  NONE
mac                                     1     1     0     0     0     0
--------------------------------------------------------------------------
  checksum                              1     1     0     0     0     0
----

If the specified triple(s) match more than one test, then
all tests requiring input are passed the same input stream.

.pass the same input to multiple tests
----
# ktest run -i ~/ssh.pcap mac: ktest:
RESULT LINE  MODULE      SUITE           TEST
NONE   0     ktest       selftest        ktest_st_none_test
PASS   24    ktest       selftest        ktest_st_pass_test
FAIL   36    ktest       selftest        ktest_st_fail_test
ERROR  66    ktest       selftest        ktest_st_err_test
SKIP   75    ktest       selftest        ktest_st_skip_test
PASS   98    ktest       selftest        ktest_st_input_test
ERROR  109   ktest       selftest        ktest_st_mult_result_test
PASS   247   mac         checksum        mac_sw_cksum_ipv4_tcp_test
PASS   302   mac         checksum        mac_sw_cksum_ipv4_bad_proto_test
PASS   627   mac         checksum        mac_sw_cksum_ipv4_snoop_test


MODULE/SUITE                            TOTAL PASS  FAIL  ERR   SKIP  NONE
ktest                                   7     2     1     2     1     1
--------------------------------------------------------------------------
  selftest                              7     2     1     2     1     1

mac                                     3     3     0     0     0     0
--------------------------------------------------------------------------
  checksum                              3     3     0     0     0     0
----

Here we pass the `ssh.pcap` stream to any test matching the `mac:` or
`ktest:` triple that requires input. A matching test that _does not_
require input runs as normal. This option is useful when you have a
suite of tests that verify different aspects of a system against the
same input.

Sometimes the line number alone is not enough to determine why an
assertion failed. In those cases you can use the `-o` option to add
the `reason` field to the output.

.ktest run failure reason
----
# ktest run -o result,line,module,suite,test,reason ktest:
RESULT LINE  MODULE      SUITE           TEST                                         REASON
NONE   0     ktest       selftest        ktest_st_none_test                           --
PASS   24    ktest       selftest        ktest_st_pass_test                           --
FAIL   36    ktest       selftest        ktest_st_fail_test                           ktest_st_is_even(5) == B_TRUE (0x0 == 0x1)
ERROR  66    ktest       selftest        ktest_st_err_test                            ktest_st_pretend_func(7) == 0 (0xffffffffffffffff == 0x0)
SKIP   75    ktest       selftest        ktest_st_skip_test                           This test should be skipped.
ERROR  109   ktest       selftest        ktest_st_mult_result_test                    multiple results: prev result at line 108


MODULE/SUITE                            TOTAL PASS  FAIL  ERR   SKIP  NONE
ktest                                   6     1     1     2     1     1
--------------------------------------------------------------------------
  selftest                              6     1     1     2     1     1
----

When scripting ktest runs you may want to use the `-Hpo` set of
options to output a stable, machine-friendly, parsable output.

.ktest run in "parsable" mode
----
# ktest run -Hpo result,line,module,suite,test,input,reason -i ~/one.txt ktest:
NONE:0:ktest:selftest:ktest_st_none_test::
PASS:24:ktest:selftest:ktest_st_pass_test::
FAIL:36:ktest:selftest:ktest_st_fail_test::ktest_st_is_even(5) == B_TRUE (0x0 == 0x1)
ERROR:66:ktest:selftest:ktest_st_err_test::ktest_st_pretend_func(7) == 0 (0xffffffffffffffff == 0x0)
SKIP:75:ktest:selftest:ktest_st_skip_test::This test should be skipped.
ERROR:90:ktest:selftest:ktest_st_input_test:/export/home/rpz/one.txt:expected 4 or more bytes, got 2
ERROR:109:ktest:selftest:ktest_st_mult_result_test::multiple results\: prev result at line 108
----

==== automating ktest and `usr/src/test`

[NOTE]
====
The integration with `usr/src/test` isn't as straightforward as I
first thought. To do it "right", each ktest test should be listed
separately in the `usr/src/test` run file. This requires setting up a
new script for each individual test. This isn't so bad for a few
tests, but as the test list grows this becomes unwieldy. It would be
nice to extend the `usr/src/test` test runner so that you could
specify a test name plus command to run, like `mac_checksum_foo =
"ktest run mac:checksum:foo"`. Or extend it to have direct
understanding of how ktest works and allow some additional syntax in
the run file for supporting that. So while integration between these
two systems is totally doable, it will not be part of the initial
ktest commit.
====

The `ktest run` command is good for interactive testing, but it's also
vital for ktest to provide good scripting support for automation of
testing and integration with other facilities like `usr/src/test`.
This is where the ktest `run-file` command comes into play. The
`run-file` command uses a file to drive the test runner. Each line of
the file specifies one triple, partially or fully-qualified, along
with an optional input file to attach to all tests that match that
triple. The triple and optional input file must be separated by a
whitespace character.

.runfile example
----
mac:
mac:checksum:mac_sw_cksum_ipv4_snoop_test /var/tmp/browsing.snoop
stream:
----

Given this file we can execute `ktest` in the following manner in our
script.

----
pfexec ktest run-file -Hpo result,line,module,suite,test,input,reason ~/run-file.txt
----

If any test produces an error that is not one of `PASS` or `SKIP`,
then `ktest(8)` returns an exit code of 1 to indicate test failure.

== Related Work

There are several components in illumos already that facilitate some
of what ktest is proposing, but they are either more narrow in scope
or lack the ability to test the kernel in full like ktest can.

=== usr/src/test

This is the framework for userland testing. It provides scaffolding
for describing, organizing, running, and reporting on tests. This is
used fairly heavily by some systems to test both userland and kernel
components. Though the later testing is of course indirect, by way of
userland APIs, system calls, and ioctls. This framework is
complementary to ktest. I envision us adding tests to various sub
directories in here where the test defines a ktest runfile for that
specific subsystem and then executes it.

=== libfakekernel

This system is the closest to ktest in terms of what you can test, but
it takes the exact opposite approach in that it brings bits of the
kernel to userland for testing (as opposed to ktest which brings the
tests to the kernel). The only documentation I could find on this are
Gordon Ross's slides from illumos day 2014 <<libfakekernel>>.

This idea was based on libzpool, and allowed Nexenta to accelerate
testing efforts when working on enhancements to SMB. Importantly, it
allowed them to perform source-level debugging on the SMB kernel code,
which they found very helpful. The ktest framework, by virtue of
running in the kernel, will not offer such a feature, but one thing I
would love to see is adding source-level debugging to mdb (perhaps a
future IPD).

The challenges with this approach are that you need to make sure to
bring over all of the DDI/DKI that your kernel module requires, into
userland. This API then needs to be emulated in some way, which may or
may not be straightforward, depending on the nature of the API. Then
you need to bring over your module-under-test into userland as well, I
believe duplicating the code and perhaps tweaking it to work as a user
library? Honestly I'm a bit unclear on how much effort this is but
looking at SMB it appears there is a `libfk...` version for many of
the `uts` files. Finally, I also wonder if there are differences in
compilation to consider here. That is, if you want to make sure your
test is executed precisely how it would be executed inside the kernel,
I wonder if differences in compilation (compiler, flags, etc.) could
cause edge cases here.

The ktest facility avoids this additional work, and potential edge
cases, by placing the test in actual kernel context, compiled as any
other kernel module would be. The main thing you lose is source-level
debugging, and for that you should continue to use libfakekernel.

So while these two overlap a lot they take fundamentally different
approaches, and I think both are useful. Also, there is no reason to
convert anything currently using libfakekernel. The work was already
done, it already exists, and it's useful to those who use it. There's
no reason both can't exist.

=== simnet

The simnet device provides a pseudo mac device (also known as a mac
provider). This is a device that implements the mac(9E) interface but
is purely virtual and allows user configuration via the `dladm(8)`
command. This is a very powerful device when combined with bridges, IP
routing, and zones, because it allows full emulating of an arbitrary
network on one host. However, this is obviously a very specialized
form of testing. It is complementary to ktest. Unfortunately we
currently don't document simnet, but you can find out more at my blog
<<resurrect-simnet>> <<simnet-basics>>.

=== Internet Packet Disturber (IPD)

The internet packet disturber (or `ipd` for short) is a little known
tool created by Robert Mustacchi. It is used to simulate congested and
lossy networks where they don't actually exist. This allows one to
test how upper layer connection-based protocols, like TCP, handle a
lossy network. Useful for testing say TCP congestion algorithms and
retransmit behavior. It's also useful to see how any application-layer
protocols react to such a network. Once again, this is a specialized
testing tool which is complementary to ktest.

To find out more see Robert's lovely big-theory statement on ipd
<<ipd-theory>> and see the ipdadm(8) man page <<ipdadm>>.

=== pshot: Pseudo Bus Nexus Driver

This is a pseudo device that allows one to create an arbitrarily
complex device tree. It looks like this tool was created by Garrett
D'Amore and provides something similar, in spirit, to simnet, but
instead targets PCI devices. Once again, this feels like a
complementary tool.

=== Miscellaneous

It seems there are several other miscellaneous test drivers, such as
`gen_drv` (Generic Character Device) and `emul64`, which I did not dig
further into. In fact, it appears there is a package called
`/system/io/tests` that consolidates many of these drivers, including
the aforementioned pshot. If someone wants to give me the skinny on
this package and its drivers I'd love to know more. That said, I don't
think any of these things overlap with ktest, and I also don't think
ktest should be delivered as part of this package. Rather, I think it
should have its own.

== References

* libfakekernel[[libfakekernel]]: https://www.slideshare.net/gordonross/illumos-day-smb2
* resurrect-simnet[[resurrect-simnet]]: https://zinascii.com/2019/resurrecting-simnet.html
* simnet-basics[[simnet-basics]]: https://zinascii.com/2019/simnet-basics.html
* ipd-theory[[ipd-theory]]: https://github.com/illumos/illumos-gate/blob/master/usr/src/uts/common/inet/ipd/ipd.c#L16
* ipadm[[ipadm]]: https://illumos.org/man/8/ipdadm
